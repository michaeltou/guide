
sudo addgroup hadoop

 

 sudo adduser -ingroup hadoop hadoop

 输入：sudo gedit /etc/sudoers

 回车，打开sudoers文件

 给hadoop用户赋予和root用户同样的权限



三、安装ssh

sudo apt-get install openssh-server
  
安装完成后，启动服务

sudo /etc/init.d/ssh start
 
查看服务是否正确启动：ps -e | grep ssh
 

设置免密码登录，生成私钥和公钥

ssh-keygen -t rsa -P ""

 

 

 

 

 

 

 

 

 

 

 

 

 

此时会在／home／hadoop/.ssh下生成两个文件：id_rsa和id_rsa.pub，前者为私钥，后者为公钥。

下面我们将公钥追加到authorized_keys中，它用户保存所有允许以当前用户身份登录到ssh客户端用户的公钥内容。

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys






1:

namenode url:
http://michaelhost:50070/dfshealth.html#tab-overview



2:

 resource manager:
http://michaelhost:8088/





3: spark

http://michaelhost:8080 


run:
spark-shell --master spark://michaelhost:7077 --executor-memory 500m

./spark-shell --master spark://michaelhost:7077 --executor-memory 512m --driver-memory 500m


job1:

sc.textFile("hdfs://michaelhost:9000/user/hadoop/testdata/core-site.xml").flatMap(_.split(" ")).map(x=>(x,1)).reduceByKey(_+_).map(x=>(x._2,x._1)).sortByKey(false).map(x=>(x._2,x._1)).take(10)



job2:

scala>val rdd=sc.textFile("hdfs://michaelhost:9000/user/hadoop/testdata/core-site.xml")
scala>rdd.cache()
scala>val wordcount=rdd.flatMap(_.split(" ")).map(x=>(x,1)).reduceByKey(_+_)
scala>wordcount.take(10)
scala>val wordsort=wordcount.map(x=>(x._2,x._1)).sortByKey(false).map(x=>(x._2,x._1))
scala>wordsort.take(10)



job3:

./spark-submit --master spark://michaelhost:7077 --class org.apache.spark.examples.SparkPi --executor-memory 512m ../lib/spark-examples-1.1.0-hadoop2.2.0.jar 200

